import tokenizer from '../../domain/justify/tokenizer'

describe('tokenizer tests', () => {

  // test 1 : whitespace hell
  // VÃ©rifie que tokenizer dÃ©coupe correctement sur tous les types de sÃ©parateurs
  // (espaces multiples, tabs, retours Ã  la ligne) et ne produit jamais de token vide
  test('1) tokenizer handles mixed whitespace correctly', () => {
    const input = 'This  is\tan\nexample.\r\nNew line here.\t\tMultiple spaces.';
    const expectedTokens = [
      'This',
      'is',
      'an',
      'example.',
      'New',
      'line',
      'here.',
      'Multiple',
      'spaces.',
    ];

    expect(tokenizer(input)).toEqual(expectedTokens);
  });

  // test 2 : ponctuation collÃ©e aux mots
  // VÃ©rifie que la ponctuation reste attachÃ©e au mot
  test('2) tokenizer keeps punctuation attached to words', () => {
    const input = 'Hello, world! This is a test.';
    const expectedTokens = [
      'Hello,',
      'world!',
      'This',
      'is',
      'a',
      'test.',
    ];

    expect(tokenizer(input)).toEqual(expectedTokens);
  });

  // test 3 : mot trÃ¨s long (> 80 caractÃ¨res)
  // VÃ©rifie que le tokenizer ne coupe pas les mots longs
  // (la rÃ¨gle des 80 caractÃ¨res est gÃ©rÃ©e par la justification, pas ici)
  test('3) tokenizer does not split very long words', () => {
    const input = 'A'.repeat(100);
    const expectedTokens = [input];

    expect(tokenizer(input)).toEqual(expectedTokens);
  });

  // test 4 : input vide
  // VÃ©rifie qu'une chaÃ®ne vide retourne un tableau vide
  test('4) tokenizer handles empty input', () => {
    const input = '';
    const expectedTokens: string[] = [];

    expect(tokenizer(input)).toEqual(expectedTokens);
  });

  // test 5 : input composÃ© uniquement de whitespace
  // VÃ©rifie que le tokenizer ne retourne pas de tokens vides
  test('5) tokenizer returns empty array for whitespace-only input', () => {
    const input = '   \t\n\r\n   ';
    const expectedTokens: string[] = [];

    expect(tokenizer(input)).toEqual(expectedTokens);
  });

  // test 6 : cas minimal avec un seul mot
  // VÃ©rifie le comportement le plus simple possible
  test('6) tokenizer handles a single word', () => {
    const input = 'Word';
    const expectedTokens = ['Word'];

    expect(tokenizer(input)).toEqual(expectedTokens);
  });

  // test 7 : caractÃ¨res accentuÃ©s et unicode
  // VÃ©rifie que les accents et caractÃ¨res spÃ©ciaux ne sont pas des sÃ©parateurs
  test('7) tokenizer handles accented and unicode characters', () => {
    const input = 'Le cafÃ© est trÃ¨s bon. Les crÃªpes sont dÃ©licieuses!';
    const expectedTokens = [
      'Le',
      'cafÃ©',
      'est',
      'trÃ¨s',
      'bon.',
      'Les',
      'crÃªpes',
      'sont',
      'dÃ©licieuses!',
    ];

    expect(tokenizer(input)).toEqual(expectedTokens);
  });

  // test 8 : emojis
  // VÃ©rifie que les emojis sont traitÃ©s comme des caractÃ¨res normaux
  test('8) tokenizer handles emojis', () => {
    const input = 'Hello ðŸ‘‹ world ðŸŒ with emojis ðŸ˜€';
    const expectedTokens = [
      'Hello',
      'ðŸ‘‹',
      'world',
      'ðŸŒ',
      'with',
      'emojis',
      'ðŸ˜€',
    ];

    expect(tokenizer(input)).toEqual(expectedTokens);
  });

  // test 9 : nombres
  // VÃ©rifie que les nombres restent intacts
  test('9) tokenizer handles numbers correctly', () => {
    const input = 'The year 2024 has 365 days and 12 months.';
    const expectedTokens = [
      'The',
      'year',
      '2024',
      'has',
      '365',
      'days',
      'and',
      '12',
      'months.',
    ];

    expect(tokenizer(input)).toEqual(expectedTokens);
  });

  // test 10 : tirets et apostrophes
  // VÃ©rifie que les tirets et apostrophes restent attachÃ©s aux mots
  test('10) tokenizer keeps hyphens and apostrophes with words', () => {
    const input = "It's a self-driving car. Don't worry!";
    const expectedTokens = [
      "It's",
      'a',
      'self-driving',
      'car.',
      "Don't",
      'worry!',
    ];

    expect(tokenizer(input)).toEqual(expectedTokens);
  });

  // test 11 : whitespace en dÃ©but et fin
  // VÃ©rifie que les espaces au dÃ©but et Ã  la fin sont ignorÃ©s
  test('11) tokenizer trims leading and trailing whitespace', () => {
    const input = '   Hello world   ';
    const expectedTokens = ['Hello', 'world'];

    expect(tokenizer(input)).toEqual(expectedTokens);
  });

  // test 12 : caractÃ¨res spÃ©ciaux comme @, #, $
  // VÃ©rifie que les caractÃ¨res spÃ©ciaux restent attachÃ©s
  test('12) tokenizer handles special characters', () => {
    const input = 'Email: user@example.com #hashtag $price';
    const expectedTokens = [
      'Email:',
      'user@example.com',
      '#hashtag',
      '$price',
    ];

    expect(tokenizer(input)).toEqual(expectedTokens);
  });

  // test 13 : multiples types de whitespace consÃ©cutifs
  // VÃ©rifie qu'aucun token vide n'est crÃ©Ã©
  test('13) tokenizer handles consecutive different whitespace types', () => {
    const input = 'Word1 \t\n\r\n Word2';
    const expectedTokens = ['Word1', 'Word2'];

    expect(tokenizer(input)).toEqual(expectedTokens);
  });

  // test 14 : texte avec uniquement des tabulations
  // VÃ©rifie que les tabs sont bien traitÃ©s comme sÃ©parateurs
  test('14) tokenizer splits on tabs', () => {
    const input = 'One\tTwo\tThree';
    const expectedTokens = ['One', 'Two', 'Three'];

    expect(tokenizer(input)).toEqual(expectedTokens);
  });
});
